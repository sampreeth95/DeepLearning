{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from random import random\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate\n",
    "from keras.layers import Input, Dense, Add, Dot, Reshape, Flatten, BatchNormalization, Lambda, Softmax, Embedding, Multiply, Add\n",
    "from matplotlib import pyplot\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from time import time\n",
    "import os\n",
    "import functools\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "from keras.engine.base_layer import Layer, InputSpec\n",
    "from keras.engine import *\n",
    "from keras.legacy import interfaces\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils.generic_utils import func_dump\n",
    "from keras.utils.generic_utils import func_load\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.utils.generic_utils import has_arg\n",
    "from keras.utils import conv_utils\n",
    "from keras.models import load_model\n",
    "from random import randint, shuffle, uniform\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from random import randint, shuffle, uniform\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load inception network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inception = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currdir = 'C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\test\\\\'\n",
    "datadir = 'C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\datasets\\\\cezanne2photo\\\\'\n",
    "Disc_learningrate = 2e-4\n",
    "Gen_learningrate = 2e-4\n",
    "batch_size = 1\n",
    "LAMBDACYCLE = 10\n",
    "LAMBDAID = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of activations from inception network for FID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FID_BATCH_SIZE = 1\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return asarray(images_list)\n",
    "\n",
    "def inception_activations(images):\n",
    "    size = 299\n",
    "    images = images.astype('float32')\n",
    "    images = scale_images(images, (299,299,3))\n",
    "    images = preprocess_input(images)\n",
    "    activations = inception.predict(images)\n",
    "    return activations\n",
    "## Batched FID because of my memory constraints\n",
    "def get_inception_activations(inps):\n",
    "    n_batches = int(np.ceil(float(inps.shape[0]) / FID_BATCH_SIZE))\n",
    "    act = np.zeros([inps.shape[0], 2048], dtype = np.float32)\n",
    "    for i in range(n_batches):\n",
    "        inp = inps[i * FID_BATCH_SIZE : (i + 1) * FID_BATCH_SIZE]\n",
    "        act[i * FID_BATCH_SIZE : i * FID_BATCH_SIZE + min(FID_BATCH_SIZE, inp.shape[0])] = inception_activations(inp)\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidCalculate(g_func, testB, test_mean, test_sigma):\n",
    "    X_out = g_func.predict(testB)\n",
    "    X_out = (X_out + 1.) * 127.5\n",
    "    # print(X_out.shape)\n",
    "    f2 = get_inception_activations(X_out)\n",
    "    mean2, sigma2 = f2.mean(axis=0), np.cov(f2, rowvar=False)\n",
    "    sum_sq_diff = np.sum((test_mean - mean2)**2)\n",
    "    cov_mean = sqrtm(test_sigma.dot(sigma2))\n",
    "    if np.iscomplexobj(cov_mean):\n",
    "        cov_mean = cov_mean.real\n",
    "    fid = sum_sq_diff + np.trace(test_sigma + sigma2 - 2.0*cov_mean)\n",
    "    print(fid)\n",
    "    with open(currdir + \"logs/\"+\"FID_LOGS\" + \".txt\", \"a\") as f:\n",
    "        f.write(str(fid) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(step, g_model_AtoB, g_model_BtoA, d_model_A, d_model_B):\n",
    "\n",
    "    filename1 = currdir + \"models\\\\\" + 'g_model_AtoB_weights_%06d.h5' % (step+1)\n",
    "    g_model_AtoB.save_weights(filename1)\n",
    "    \n",
    "    filename2 = currdir + \"models\\\\\" + 'g_model_BtoA_weights_%06d.h5' % (step+1)\n",
    "    g_model_BtoA.save_weights(filename2)\n",
    "    \n",
    "    filename3 = currdir + \"models\\\\\" + 'd_model_A_weights_%06d.h5' % (step+1)\n",
    "    d_model_A.save_weights(filename3)\n",
    "    \n",
    "    filename4 =  currdir + \"models\\\\\" + 'd_model_B_weights_%06d.h5' % (step+1)\n",
    "    d_model_B.save_weights(filename4)\n",
    "    print('>Saved: models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading images and preprocessing(Random cropping and flipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageprocess(img_path):   \n",
    "    img = image.load_img(img_path)\n",
    "    img = img.resize((143, 143), Image.BILINEAR) # To perform random cropping of size 15\n",
    "    img_arr = image.img_to_array(img)\n",
    "    img_norm = np.array(img_arr)/255*2-1\n",
    "    h1 = (143 - 128)//2\n",
    "    h2 = (143 + 128)//2\n",
    "    shift = randint(0,h1)\n",
    "    h1 = h1 - shift\n",
    "    h2 = h2 - shift\n",
    "    w1 = h1\n",
    "    w2 = h2\n",
    "    img_cropped = img_norm[h1:h2,w1:w2,:]\n",
    "    flip = randint(0,1)\n",
    "    if flip:\n",
    "        img_cropped = img_cropped[:,::-1]\n",
    "    return img_cropped\n",
    "\n",
    "def loadimage(path):\n",
    "    train_A_paths = glob.glob(path +  \"trainA/*.jpg\")\n",
    "    train_B_paths = glob.glob(path +  \"trainB/*.jpg\")\n",
    "    test_A_paths = glob.glob(path +  \"testA/*.jpg\")\n",
    "    test_B_paths = glob.glob(path +  \"testB/*.jpg\")\n",
    "    \n",
    "    data = []\n",
    "    for img_path in train_A_paths:\n",
    "        data.append(imageprocess(img_path))\n",
    "    train_A = np.float32(data)\n",
    "    print(train_A.shape)\n",
    "    del data\n",
    "\n",
    "    data = []\n",
    "    for img_path in train_B_paths:\n",
    "        data.append(imageprocess(img_path))\n",
    "    train_B = np.float32(data)\n",
    "    del data\n",
    "    \n",
    "    data = []\n",
    "    for img_path in test_A_paths:\n",
    "        data.append(imageprocess(img_path))\n",
    "    test_A = np.float32(data)\n",
    "    del data\n",
    "    \n",
    "    data = []\n",
    "    for img_path in test_B_paths:\n",
    "        data.append(imageprocess(img_path))\n",
    "    test_B = np.float32(data)\n",
    "    del data\n",
    "    \n",
    "    return train_A, train_B, test_A, test_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A, train_B, test_A, test_B = loadimage(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean and sigma for A domain(artistic) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_pre = vstack(((train_A + 1.)*127.5, (test_A + 1.)*127.5))\n",
    "f1 = get_inception_activations(fid_pre)\n",
    "test_mean, test_sigma = f1.mean(axis=0), np.cov(f1, rowvar=False)\n",
    "del fid_pre, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape = (128, 128, 3)):\n",
    "   \n",
    "    init = 'glorot_uniform'\n",
    "   \n",
    "    in_image = Input(shape=image_shape)\n",
    "    \n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(d)\n",
    "    d = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(d, training=1)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "   \n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(d)\n",
    "    d = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(d, training=1)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(d)\n",
    "    d = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(d, training=1)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init, use_bias = False)(d)\n",
    "    d = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(d, training=1)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    \n",
    "    model = Model(in_image, patch_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(n_filters, input_layer):\n",
    "    \n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init, use_bias = False)(input_layer)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    g = Activation('relu')(g)\n",
    "    \n",
    "    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init, use_bias = False)(g)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    \n",
    "    g = Add()([g, input_layer])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(image_shape=(128,128,3), n_resnet=6):\n",
    "    \n",
    "    init = 'glorot_uniform'\n",
    "   \n",
    "    in_image = Input(shape=image_shape)\n",
    "    \n",
    "    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init, use_bias = False)(in_image)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    g = Activation('relu')(g)\n",
    "   \n",
    "    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(g)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    g = Activation('relu')(g)\n",
    "    \n",
    "    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(g)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    g = Activation('relu')(g)\n",
    "    \n",
    "    for _ in range(n_resnet):\n",
    "        g = resnet_block(256, g)\n",
    "   \n",
    "    g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(g)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    g = Activation('relu')(g)\n",
    "    \n",
    "    g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init, use_bias = False)(g)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    g = Activation('relu')(g)\n",
    "    \n",
    "    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init, use_bias = False)(g)\n",
    "    g = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(g, training=1)\n",
    "    out_image = Activation('tanh')(g)\n",
    "    \n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Disc_A = define_discriminator(image_shape=(128,128,3))\n",
    "Disc_B = define_discriminator(image_shape=(128,128,3))\n",
    "Disc_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen_AtoB = define_generator(image_shape=(128,128,3))\n",
    "Gen_BtoA = define_generator(image_shape=(128,128,3))\n",
    "Gen_AtoB.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss Functions : Generator Loss, Discriminator Loss, Cycle Loss, Identity Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = lambda output, target : K.mean(K.abs(K.square(output-target)))\n",
    "\n",
    "fake_pool_a = K.placeholder(shape=(None, 128, 128, 3))\n",
    "fake_pool_b = K.placeholder(shape=(None, 128, 128, 3))\n",
    "\n",
    "real_A = Gen_AtoB.inputs[0]\n",
    "real_B = Gen_BtoA.inputs[0]\n",
    "\n",
    "fake_B = Gen_AtoB.outputs[0]\n",
    "fake_A = Gen_BtoA.outputs[0]\n",
    "\n",
    "rec_A = Gen_BtoA([fake_B])\n",
    "rec_B = Gen_AtoB([fake_A])\n",
    "\n",
    "cycle_ABA = K.function([real_A], [fake_B, rec_A])\n",
    "cycle_BAB = K.function([real_B], [fake_A, rec_B])\n",
    "\n",
    "\n",
    "Disc_op_real_A = Disc_A([real_A])\n",
    "Disc_op_fake_A = Disc_A([fake_A])\n",
    "Disc_op_fakepool_A = Disc_A([fake_pool_a])\n",
    "\n",
    "Disc_A_real_loss = loss_fn(Disc_op_real_A, K.ones_like(Disc_op_real_A))\n",
    "Disc_A_fake_loss = loss_fn(Disc_op_fakepool_A, K.zeros_like(Disc_op_fakepool_A))\n",
    "\n",
    "Disc_A_loss = Disc_A_real_loss + Disc_A_fake_loss\n",
    "\n",
    "Disc_op_real_B = Disc_B([real_B])\n",
    "Disc_op_fake_B = Disc_B([fake_B])\n",
    "Disc_op_fakepool_B = Disc_B([fake_pool_b])\n",
    "\n",
    "Disc_B_real_loss = loss_fn(Disc_op_real_B, K.ones_like(Disc_op_real_B))\n",
    "Disc_B_fake_loss = loss_fn(Disc_op_fakepool_B, K.zeros_like(Disc_op_fakepool_B))\n",
    "\n",
    "Disc_B_loss = Disc_B_real_loss + Disc_B_fake_loss\n",
    "\n",
    "Gen_BtoA_loss = loss_fn(Disc_op_fake_A, K.ones_like(Disc_op_fake_A))\n",
    "Gen_AtoB_loss = loss_fn(Disc_op_fake_B, K.ones_like(Disc_op_fake_B))\n",
    "\n",
    "loss_cycle_A = K.mean(K.abs(rec_A-real_A))\n",
    "loss_cycle_B = K.mean(K.abs(rec_B-real_B))\n",
    "\n",
    "tot_loss_cycle = loss_cycle_A + loss_cycle_B\n",
    "\n",
    "id_A = Gen_BtoA([real_A])\n",
    "loss_id_A = K.mean(K.abs(id_A - real_A))\n",
    "\n",
    "id_B = Gen_AtoB([real_B])\n",
    "loss_id_B = K.mean(K.abs(id_B - real_B))\n",
    "\n",
    "tot_loss_id = loss_id_A + loss_id_B\n",
    "\n",
    "tot_loss_D = Disc_A_loss + Disc_B_loss\n",
    "\n",
    "tot_loss_G = Gen_BtoA_loss + Gen_AtoB_loss + LAMBDACYCLE*tot_loss_cycle + LAMBDAID*tot_loss_id\n",
    "\n",
    "weightsD = Disc_A.trainable_weights + Disc_B.trainable_weights\n",
    "weightsG = Gen_AtoB.trainable_weights + Gen_BtoA.trainable_weights\n",
    "\n",
    "training_updates_disc = Adam(lr=Disc_learningrate, beta_1=0.5).get_updates(weightsD,[],tot_loss_D)\n",
    "Disc_train = K.function([real_A, real_B, fake_pool_a, fake_pool_b],[Disc_A_loss, Disc_B_loss], training_updates_disc)\n",
    "\n",
    "training_updates_gen = Adam(lr=Gen_learningrate, beta_1=0.5).get_updates(weightsG,[], tot_loss_G)\n",
    "Gen_train = K.function([real_A, real_B, fake_pool_a, fake_pool_b], [Gen_BtoA_loss, Gen_AtoB_loss, tot_loss_cycle, tot_loss_id], training_updates_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from io import BytesIO\n",
    "byte_io = BytesIO()\n",
    "def display_image(X, rows=1, iteration=1, sv = False):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ((X+1.)*127.5).clip(0,255).astype('uint8')\n",
    "    int_X = int_X.reshape(rows, -1, 128, 128,3).swapaxes(1,2).reshape(rows*128,-1, 3)\n",
    "    img = Image.fromarray(int_X)\n",
    "    display(img)\n",
    "    if sv:\n",
    "        img.save(currdir + \"images/\" + \"{}.png\".format(iteration),\"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(A,B, iteration = 0, sv = False):\n",
    "    assert A.shape==B.shape\n",
    "    def G(fn_generate, X, Y):\n",
    "        r = np.array([fn_generate([X[i:i+1]]) for i in range(X.shape[0])])\n",
    "        return r.swapaxes(0,1)[:,:,0]        \n",
    "    rA = G(cycle_ABA, A, B)\n",
    "    rB = G(cycle_BAB, B, A)\n",
    "    arr = np.concatenate([A,B,rA[0],rB[0],rA[1],rB[1]])\n",
    "    display_image(arr, 3, iteration, sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "t0 = time.time()\n",
    "MAXEPOCHS = 100\n",
    "steps = 0\n",
    "epoch = 0\n",
    "localepoch1 = 0\n",
    "localepoch2 = 0\n",
    "err_Gen_BtoA = 0\n",
    "err_Gen_AtoB = 0\n",
    "err_Disc_A = 0\n",
    "err_Disc_B = 0\n",
    "err_cycle = 0\n",
    "err_id = 0\n",
    "display_iters = 50\n",
    "fid_freq = 1000\n",
    "save_freq = 7000\n",
    "save_image_iters = 3000\n",
    "j = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "fake_A_pool = []\n",
    "fake_B_pool = []\n",
    "\n",
    "while epoch < MAXEPOCHS:   \n",
    "    \n",
    "    if j+batch_size > len(train_A):\n",
    "        j = 0\n",
    "        localepoch1 += 1\n",
    "        np.random.shuffle(train_A)\n",
    "    A = train_A[j:j+batch_size]\n",
    "    j += batch_size\n",
    "\n",
    "    if i+batch_size > len(train_B):\n",
    "        i = 0\n",
    "        localepoch2 += 1\n",
    "        np.random.shuffle(train_B)\n",
    "    B = train_B[i:i+batch_size]\n",
    "    i += batch_size\n",
    "    \n",
    "    epoch = min(localepoch1, localepoch2)\n",
    "#     print(A.shape)\n",
    "\n",
    "    tmp_fake_A = Gen_BtoA.predict(B)\n",
    "    tmp_fake_B = Gen_AtoB.predict(A) \n",
    "\n",
    "    tmp_A = []\n",
    "    tmp_B = []\n",
    "    \n",
    "    for img in tmp_fake_A:\n",
    "        if len(fake_A_pool) < 50:\n",
    "            fake_A_pool.append(img)\n",
    "            tmp_A.append(img)\n",
    "        else:\n",
    "            p = np.random.uniform(0, 1)\n",
    "            if p > 0.5:\n",
    "                random_id = randint(0, 49)\n",
    "                tmp = np.copy(fake_A_pool[random_id])\n",
    "                fake_A_pool[random_id] = img\n",
    "                tmp_A.append(tmp)\n",
    "            else:\n",
    "                tmp_A.append(img)\n",
    "                \n",
    "    for img in tmp_fake_B:\n",
    "        if len(fake_B_pool) < 50:\n",
    "            fake_B_pool.append(img)\n",
    "            tmp_B.append(img)\n",
    "        else:\n",
    "            p = np.random.uniform(0, 1)\n",
    "            if p > 0.5:\n",
    "                random_id = randint(0, 49)\n",
    "                tmp = np.copy(fake_B_pool[random_id])\n",
    "                fake_B_pool[random_id] = img\n",
    "                tmp_B.append(tmp)\n",
    "            else:\n",
    "                tmp_B.append(img) \n",
    "    \n",
    "    pool_a = np.array(tmp_A)\n",
    "    pool_b = np.array(tmp_B)\n",
    "           \n",
    "    err_Disc_A, err_Disc_B = Disc_train([A, B, pool_a, pool_b])\n",
    "  \n",
    "    err_Gen_BtoA, err_Gen_AtoB, err_cycle, err_id = Gen_train([A, B, pool_a, pool_b])\n",
    "    steps+=1\n",
    "    \n",
    "    with open(currdir + \"logs/\" + \"losses_logs\" + \".txt\", \"a\") as f:\n",
    "        f.write(str(err_Disc_A) + '\\t' + str(err_Disc_B) + '\\t' + str(err_Gen_BtoA) + '\\t' + str(err_Gen_AtoB) + '\\t' +  str(err_cycle) + '\\t' + str(err_id) + '\\n')\n",
    "    print(\"err_Disc_A: {}\".format(err_Disc_A) + \" err_Disc_B: {}\".format(err_Disc_B) + \" err_Gen_BtoA: {}\".format(err_Gen_BtoA) + \" err_Gen_AtoB: {}\".format(err_Gen_AtoB))    \n",
    "        \n",
    "    \n",
    "    if steps%fid_freq == 0:\n",
    "        print(time.time()-t0)\n",
    "        print(\"Epoch-{}\".format(epoch))\n",
    "        print(\"err_Disc_A: {}\".format(err_Disc_A) + \" err_Disc_B: {}\".format(err_Disc_B) + \" err_Gen_BtoA: {}\".format(err_Gen_BtoA) + \" err_Gen_AtoB: {}\".format(err_Gen_AtoB))\n",
    "        idx = randint(0,len(test_B)-100)\n",
    "        fid_B_data = test_B[idx:idx+100]\n",
    "        fidCalculate(Gen_BtoA, fid_B_data, test_mean, test_sigma)      \n",
    "\n",
    "    if steps%display_iters==0:\n",
    "        clear_output()\n",
    "        \n",
    "        if j+4 > len(train_A):\n",
    "            j = 0\n",
    "            localepoch1 += 1\n",
    "            np.random.shuffle(train_A)\n",
    "        A = train_A[j:j+4]\n",
    "        j += 4\n",
    "\n",
    "        if i+4 > len(train_B):\n",
    "            i = 0\n",
    "            localepoch2 += 1\n",
    "            np.random.shuffle(train_B)\n",
    "        B = train_B[i:i+4]\n",
    "        i += 4        \n",
    "        \n",
    "        if(steps%save_image_iters == 0):\n",
    "            gen_image(A, B, steps, sv=True)\n",
    "        else:\n",
    "            gen_image(A,B, steps)    \n",
    "    \n",
    "   \n",
    "    if steps%save_freq == 0:\n",
    "        save_models(steps, Gen_AtoB, Gen_BtoA, Disc_A, Disc_B)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = randint(0, len(train_B))\n",
    "B = train_B[idx:idx+1]\n",
    "def G(fn_generate, X):\n",
    "    r = np.array([fn_generate([X[i:i+1]]) for i in range(X.shape[0])])\n",
    "#         print(r.shape)\n",
    "    return r.swapaxes(0,1)[:,:,0]\n",
    "Gen_BtoA.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'g_model_BtoA_weights_280001.h5')\n",
    "Gen_AtoB.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'g_model_AtoB_weights_280001.h5')\n",
    "Disc_A.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'd_model_A_weights_280001.h5')\n",
    "Disc_B.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'd_model_B_weights_280001.h5')    \n",
    "r_cezanne = G(cycle_BAB, B)\n",
    "arr = np.concatenate([B, r_cezanne[0]])\n",
    "display_image(arr, 1)\n",
    "\n",
    "\n",
    "idx_fid = randint(0,len(test_B)-100)\n",
    "fid_B_data = test_B[idx_fid:idx_fid+100]\n",
    "print(\"FID:\")\n",
    "fidCalculate(Gen_BtoA, fid_B_data, test_mean, test_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell for generating all artistic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = randint(0, len(train_B))\n",
    "B = train_B[idx:idx+1]\n",
    "\n",
    "def G(fn_generate, X):\n",
    "    r = np.array([fn_generate([X[i:i+1]]) for i in range(X.shape[0])])\n",
    "    return r.swapaxes(0,1)[:,:,0]\n",
    "\n",
    "Gen_BtoA.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\ukiyoe\\\\finalweights\\\\' + 'g_model_BtoA_weights_360001.h5')\n",
    "Gen_AtoB.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\ukiyoe\\\\finalweights\\\\' + 'g_model_AtoB_weights_360001.h5')\n",
    "Disc_A.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\ukiyoe\\\\finalweights\\\\' + 'd_model_A_weights_360001.h5')\n",
    "Disc_B.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\ukiyoe\\\\finalweights\\\\' + 'd_model_B_weights_360001.h5')\n",
    "r_ukiyoe = G(cycle_BAB, B)\n",
    "\n",
    "Gen_BtoA.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\vangogh\\\\finalweights\\\\' + 'g_model_BtoA_weights_150001.h5')\n",
    "Gen_AtoB.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\vangogh\\\\finalweights\\\\' + 'g_model_AtoB_weights_150001.h5')\n",
    "Disc_A.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\vangogh\\\\finalweights\\\\' + 'd_model_A_weights_150001.h5')\n",
    "Disc_B.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\vangogh\\\\finalweights\\\\' + 'd_model_B_weights_150001.h5')    \n",
    "r_vangogh = G(cycle_BAB, B)\n",
    "\n",
    "Gen_BtoA.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\monet\\\\finalweights\\\\' + 'g_model_BtoA_weights_024001.h5')\n",
    "Gen_AtoB.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\monet\\\\finalweights\\\\' + 'g_model_AtoB_weights_024001.h5')\n",
    "Disc_A.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\monet\\\\finalweights\\\\' + 'd_model_A_weights_024001.h5')\n",
    "Disc_B.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\monet\\\\finalweights\\\\' + 'd_model_B_weights_024001.h5')    \n",
    "r_monet = G(cycle_BAB, B)\n",
    "\n",
    "Gen_BtoA.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'g_model_BtoA_weights_280001.h5')\n",
    "Gen_AtoB.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'g_model_AtoB_weights_280001.h5')\n",
    "Disc_A.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'd_model_A_weights_280001.h5')\n",
    "Disc_B.load_weights('C:\\\\Users\\\\parit\\\\Documents\\\\CycleGAN\\\\CycleGANRestNetGen\\\\cezanne\\\\finalweights\\\\' + 'd_model_B_weights_280001.h5')    \n",
    "r_cezanne = G(cycle_BAB, B)\n",
    "\n",
    "arr = np.concatenate([B,r_ukiyoe[0], r_vangogh[0], r_monet[0], r_cezanne[0]])\n",
    "print(\"   real image     Ukiyoe image     Vangogh Image     Monet image      Cezanne Image\")                          \n",
    "display_image(arr, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://github.com/tjwei/GANotebooks/blob/master/CycleGAN-keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
